Train set size: 6370
Validation set size: 1593
Test set size: 1991

[I 2025-05-04 22:44:22,353] A new study created in memory with name: no-name-c05064d1-6c53-4b49-b1f9-db2f34221642

Starting hyperparameter tuning trial with lr=0.0004009126047122678, batch_size=32, optimizer=SGD, freeze_layers=True
Trial Epoch 1/3 - Train Loss: 2.0733, Train Acc: 0.2945 - Val Loss: 1.7332, Val Acc: 0.5009
Trial Epoch 2/3 - Train Loss: 1.6307, Train Acc: 0.4995 - Val Loss: 1.4632, Val Acc: 0.5650
Trial Epoch 3/3 - Train Loss: 1.4137, Train Acc: 0.5683 - Val Loss: 1.3079, Val Acc: 0.6202
[I 2025-05-04 22:46:36,171] Trial 0 finished with value: 1.3079231612276747 and parameters: {'lr': 0.0004009126047122678, 'batch_size': 32, 'optimizer': 'SGD', 'freeze_layers': True}. Best is trial 0 with value: 1.3079231612276747.

Starting hyperparameter tuning trial with lr=0.0006646772590408398, batch_size=32, optimizer=Adam, freeze_layers=True
Trial Epoch 1/3 - Train Loss: 1.6389, Train Acc: 0.4597 - Val Loss: 1.2136, Val Acc: 0.6146
Trial Epoch 2/3 - Train Loss: 1.1336, Train Acc: 0.6323 - Val Loss: 0.9940, Val Acc: 0.6654
Trial Epoch 3/3 - Train Loss: 0.9475, Train Acc: 0.6959 - Val Loss: 0.8855, Val Acc: 0.7106
[I 2025-05-04 22:48:47,779] Trial 1 finished with value: 0.8855196993005253 and parameters: {'lr': 0.0006646772590408398, 'batch_size': 32, 'optimizer': 'Adam', 'freeze_layers': True}. Best is trial 1 with value: 0.8855196993005253.

Starting hyperparameter tuning trial with lr=0.0008418965502492345, batch_size=32, optimizer=Adam, freeze_layers=True
Trial Epoch 1/3 - Train Loss: 1.5526, Train Acc: 0.4816 - Val Loss: 1.1328, Val Acc: 0.6390
Trial Epoch 2/3 - Train Loss: 1.0557, Train Acc: 0.6513 - Val Loss: 0.9356, Val Acc: 0.6880
Trial Epoch 3/3 - Train Loss: 0.9217, Train Acc: 0.6865 - Val Loss: 0.8450, Val Acc: 0.7100
[I 2025-05-04 22:50:59,468] Trial 2 finished with value: 0.8450356495133452 and parameters: {'lr': 0.0008418965502492345, 'batch_size': 32, 'optimizer': 'Adam', 'freeze_layers': True}. Best is trial 2 with value: 0.8450356495133452.

Starting hyperparameter tuning trial with lr=0.002215443806769084, batch_size=32, optimizer=Adam, freeze_layers=True
Trial Epoch 1/3 - Train Loss: 1.3744, Train Acc: 0.5193 - Val Loss: 0.9076, Val Acc: 0.6868
Trial Epoch 2/3 - Train Loss: 0.9447, Train Acc: 0.6728 - Val Loss: 0.8125, Val Acc: 0.7037
Trial Epoch 3/3 - Train Loss: 0.8396, Train Acc: 0.7028 - Val Loss: 0.8331, Val Acc: 0.7081
[I 2025-05-04 22:53:11,267] Trial 3 finished with value: 0.8124565582416077 and parameters: {'lr': 0.002215443806769084, 'batch_size': 32, 'optimizer': 'Adam', 'freeze_layers': True}. Best is trial 3 with value: 0.8124565582416077.

Starting hyperparameter tuning trial with lr=0.0003534032482373093, batch_size=32, optimizer=SGD, freeze_layers=False
Trial Epoch 1/3 - Train Loss: 2.0560, Train Acc: 0.3113 - Val Loss: 1.6904, Val Acc: 0.4959
Trial Epoch 2/3 - Train Loss: 1.4980, Train Acc: 0.5380 - Val Loss: 1.2611, Val Acc: 0.6290
Trial Epoch 3/3 - Train Loss: 1.1768, Train Acc: 0.6436 - Val Loss: 1.0172, Val Acc: 0.7075
[I 2025-05-04 22:57:43,227] Trial 4 finished with value: 1.0172310032263838 and parameters: {'lr': 0.0003534032482373093, 'batch_size': 32, 'optimizer': 'SGD', 'freeze_layers': False}. Best is trial 3 with value: 0.8124565582416077.

Best hyperparameters found: {'lr': 0.002215443806769084, 'batch_size': 32, 'optimizer': 'Adam', 'freeze_layers': True}
